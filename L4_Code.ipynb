{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending a GET request to http://localhost:5000\n",
    "r.get('http://localhost:5000').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a shopping list via the /shoppingList endpoint on http://localhost:5000\n",
    "\n",
    "# DELETE method without any data passed will drop the whole shopping list\n",
    "r.delete('http://localhost:5000/shoppingList')\n",
    "\n",
    "# GET method retrieves the (as of now empty) shopping list\n",
    "r.get('http://localhost:5000/shoppingList').json()\n",
    "\n",
    "# POST method creates a new entry in the shopping list\n",
    "r.post('http://localhost:5000/shoppingList', data={'entry': 'milk'})\n",
    "r.post('http://localhost:5000/shoppingList', data={'entry': 'tea'})\n",
    "r.post('http://localhost:5000/shoppingList', data={'entry': 'wine'})\n",
    "\n",
    "# POST method changes an entry in the shopping list\n",
    "r.put(\n",
    "    'http://localhost:5000/shoppingList', \n",
    "    data=json.dumps({'entry': {'wine': 'beer'}}),\n",
    "    headers={'content-type': 'application/json'}\n",
    ")\n",
    "\n",
    "r.get('http://localhost:5000/shoppingList').json()\n",
    "\n",
    "# DELETE method removes an entry\n",
    "r.delete('http://localhost:5000/shoppingList', data={'entry': 'beer'})\n",
    "\n",
    "r.get('http://localhost:5000/shoppingList').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `params` argument\n",
    "r.get('http://localhost:5000/shoppingList', params={'a': 'b', 'c': 'd'}).url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with API limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credentials.json') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "CLOCKIFY_API_KEY = creds['CLOCKIFY_API_KEY']\n",
    "CLOCKIFY_WORKSPACE = '5d6d305c8c5e57633d851e45'\n",
    "CLOCKIFY_ENDPOINT = 'https://api.clockify.me/api/v1'\n",
    "CLOCKIFY_HEADERS = {\n",
    "    'Content-type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'X-API-KEY': CLOCKIFY_API_KEY\n",
    "}\n",
    "\n",
    "ZENHUB_API_KEY = creds['ZENHUB_API_KEY']\n",
    "ZENHUB_ENDPOINT = 'https://api.zenhub.com'\n",
    "ZENHUB_HEADERS = {\n",
    "    'Content-type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'X-Authentication-Token': ZENHUB_API_KEY\n",
    "}\n",
    "\n",
    "GITHUB_API_KEY = creds['GITHUB_API_KEY']\n",
    "GITHUB_ENDPOINT = 'https://api.github.com'\n",
    "GITHUB_HEADERS = {\n",
    "    'Content-type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': f'token {GITHUB_API_KEY}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with paging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve all results from Github API for given url\n",
    "#\n",
    "# Requests the results page by page and puts them into a single list, which is then returned\n",
    "# @param url The request url excluding the common prefix of the endpoint\n",
    "# @param arg Possible additional arguments as a string, prefixed with an ampersand, for example '&state=all'\n",
    "# @return A list of dicts, each dict containing one result for given url\n",
    "def get_github(url, arg = None):\n",
    "    page = 1\n",
    "    res = []\n",
    "    ret = get_github_page(url, page, arg)\n",
    "    while len(ret) > 0:\n",
    "        res += ret\n",
    "        page += 1\n",
    "        ret = get_github_page(url, page, arg)\n",
    "    return res\n",
    "\n",
    "## Retrieve one page of results from Github API for given url\n",
    "#\n",
    "# The default limit to the size of a page returned is 30\n",
    "# @param url The request url excluding the common prefix of the endpoint\n",
    "# @param page Number of the page to retrieve\n",
    "# @param arg Possible additional arguments as a string, prefixed with an ampersand, for example '&state=all'\n",
    "# @return A list of dicts, each dict containing one result for given url\n",
    "def get_github_page(url, page, arg):\n",
    "    return r.get(\n",
    "        f'{GITHUB_ENDPOINT}/{url}?page={page}&per_page=100{arg}',\n",
    "        headers = GITHUB_HEADERS\n",
    "    ).json()\n",
    "\n",
    "## Retrieve all results from Zenhub API for given url\n",
    "#\n",
    "# @param url The request url excluding the common prefix of the endpoint\n",
    "# @param json Boolean flag indicating whether to extract the requested data to a dict, or return the whole request response\n",
    "# @return Either a list of dicts, each dict containing one result for given url, or the response to the request\n",
    "def get_zenhub(url, json = True):\n",
    "    res = r.get(\n",
    "        f'{ZENHUB_ENDPOINT}/{url}',\n",
    "        headers = ZENHUB_HEADERS\n",
    "    )\n",
    "\n",
    "    return res.json() if json else res\n",
    "\n",
    "## Retrieve all results from Clockify API for given url\n",
    "#\n",
    "# Requests the results page by page and puts them into a single list, which is then returned\n",
    "# @param url The request url excluding the common prefix of the endpoint and workspace\n",
    "# @return A list of dicts, each dict containing one result for given url\n",
    "def get_clockify(url):\n",
    "    page = 1\n",
    "    res = []\n",
    "    ret = get_clockify_page(url, page)\n",
    "    while len(ret) > 0:\n",
    "        # Clockify does not provide any information on when we can send more requests and disregards any requests in the meantime\n",
    "        # therefore we have to play it safe by strictly limiting it to 10 requests/second by sleeping for 0.1s after every request\n",
    "        time.sleep(0.1)\n",
    "        res += ret\n",
    "        page += 1\n",
    "        ret = get_clockify_page(url, page)\n",
    "    return res\n",
    "\n",
    "## Retrieve one page of results from Clockify API for given url\n",
    "#\n",
    "# The default limit to the size of a page returned is 50\n",
    "# @param url The request url excluding the common prefix of the endpoint and workspace\n",
    "# @param page Number of the page to retrieve\n",
    "# @return A list of dicts, each dict containing one result for given url\n",
    "def get_clockify_page(url, page):\n",
    "    return r.get(\n",
    "        f'{CLOCKIFY_ENDPOINT}/workspaces/{CLOCKIFY_WORKSPACE}/{url}?page={page}',\n",
    "        headers = CLOCKIFY_HEADERS\n",
    "    ).json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with limited number of requests per given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests as r\n",
    "import re\n",
    "import pandas as pd\n",
    "from treelib import Node, Tree\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temporary variable to make user retrieval more efficient\n",
    "users_tmp = get_clockify('users')\n",
    "\n",
    "## A pandas.DataFrame with id and name of all users\n",
    "users = pd.DataFrame({\n",
    "    'id': [user['id'] for user in users_tmp],\n",
    "    'name': [user['name'] for user in users_tmp]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A pandas.DataFrame with all time entries\n",
    "time_entries = pd.DataFrame()\n",
    "\n",
    "for user_id in users['id']:\n",
    "    for entry in get_clockify(f'user/{user_id}/time-entries'):\n",
    "        time_entries = time_entries.append({\n",
    "            'userId': user_id,\n",
    "            'id': entry['id'],\n",
    "            'description': entry['description'],\n",
    "            'projectId': entry['projectId'],\n",
    "            'billable': entry['billable'],\n",
    "            'start': entry['timeInterval']['start'],\n",
    "            'end': entry['timeInterval']['end']\n",
    "        }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temporary variable to make repo retrieval more efficient\n",
    "repos_json = get_github('orgs/aivero/repos')\n",
    "\n",
    "## A pandas.DataFrame containing all Github repositories\n",
    "repos = pd.DataFrame(columns=['id', 'name'],\n",
    "                 data=[[repo['id'], repo['name']] for repo in repos_json])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A pandas.DataFrame containing all issues\n",
    "issues = pd.DataFrame(\n",
    "    columns=['repo', 'repo_id', 'issue_number', 'description', 'repository_url']\n",
    ")\n",
    "\n",
    "# Retrieve issues from all repos and put them into one pandas.DataFrame\n",
    "for id, row in repos.iterrows():\n",
    "    repo = row['name']\n",
    "    repo_id = row['id']\n",
    "    issues_arr = get_github(f'repos/aivero/{repo}/issues', arg=\"&state=all\")\n",
    "\n",
    "    for issue in issues_arr:\n",
    "        issues = issues.append(\n",
    "            {\n",
    "                'repo': repo,\n",
    "                'repo_id': repo_id,\n",
    "                'description': issue['title'],\n",
    "                'issue_number': issue['number'],\n",
    "                'repository_url': issue['repository_url'],\n",
    "                'labels': ','.join([iss['name'] for iss in issue['labels']]),\n",
    "                'state': issue['state'],\n",
    "                'closed_at': issue['closed_at']\n",
    "            },\n",
    "            ignore_index=True)\n",
    "issues['closed_at'] = pd.to_datetime(issues['closed_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether an issue has children issues (is_epic = True) for all epics.\n",
    "# ZenHub does not allow more than 100 requests per minute,\n",
    "# therefore this script controls the number of requests per minute\n",
    "# and in case 100 requests per minute is reached,\n",
    "# the script sleeps until it is allowed to send a request again.\n",
    "is_epic = []\n",
    "for _, x in issues.iterrows():\n",
    "    rq = get_zenhub('p1/repositories/{}/issues/{}'.format(x['repo_id'], x['issue_number']), json=False)\n",
    "\n",
    "    # Zenhub API returns the information about the limit, so we can check when we are allowed to send requests again\n",
    "    if int(rq.headers['X-RateLimit-Used']) >= int(rq.headers['X-RateLimit-Limit']) - 1:\n",
    "        diff = int(rq.headers['X-RateLimit-Reset']) - time.time()\n",
    "        if diff >= 0:\n",
    "            print(diff)\n",
    "            time.sleep(diff)\n",
    "\n",
    "    is_epic.append(rq.json()['is_epic'])\n",
    "\n",
    "issues['is_epic'] = is_epic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with limited number of requests without headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_epic = []\n",
    "for _, x in issues.iterrows():\n",
    "    rq = get_zenhub('p1/repositories/{}/issues/{}'.format(x['repo_id'], x['issue_number']), json=False)\n",
    "    \n",
    "    while rq.status_code == 403:\n",
    "        rq = get_zenhub('p1/repositories/{}/issues/{}'.format(x['repo_id'], x['issue_number']), json=False)\n",
    "        continue\n",
    "    print(rq.status_code)\n",
    "\n",
    "issues['is_epic'] = is_epic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T12:41:11.592814Z",
     "start_time": "2020-12-02T12:41:11.585668Z"
    }
   },
   "source": [
    "## API wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyGithub wrapper for Github API\n",
    "\n",
    "from github import Github\n",
    "\n",
    "g = Github(f\"{GITHUB_API_KEY}\")\n",
    "for repo in g.get_user().get_repos():\n",
    "    print(repo.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yfinance wrapper for Yahoo Finance API\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "yf.download(\n",
    "    tickers = \"AAPL GOOG MSFT\",\n",
    "    period = 'ytd',\n",
    "    group_by = 'ticker'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_finance_api2 wrapper for Yahoo Finance API\n",
    "\n",
    "from yahoo_finance_api2 import share\n",
    "\n",
    "tickers = ['GOOG', 'AAPL', 'MSFT']\n",
    "\n",
    "for ticker in tickers:\n",
    "    t = share.Share(ticker)\n",
    "    t = t.get_historical(share.PERIOD_TYPE_DAY, 10, share.FREQUENCY_TYPE_MINUTE, 5)\n",
    "    print(t['open'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbpy\n",
    "\n",
    "api = wbpy.IndicatorAPI()\n",
    "iso_country_codes = [\"GB\", \"FR\", \"JP\"]\n",
    "total_population = \"SP.POP.TOTL\"\n",
    "\n",
    "dataset = api.get_dataset(total_population, iso_country_codes, date=\"2010:2012\")\n",
    "dataset.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "\n",
    "content = r.get('http://en.wikipedia.org/wiki/Python_(programming_language)').text\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content[content.find('infobox vevent'):]\n",
    "\n",
    "for i in range(8):\n",
    "    content = content[content.find('tr')+2:]\n",
    "    \n",
    "content = content[content.find('td')+2:]\n",
    "\n",
    "for i in range(2):\n",
    "    content = content[content.find('>')+1:]\n",
    "    \n",
    "author = content[0:content.find('<')]\n",
    "\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeatifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = r.get('http://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "\n",
    "soup = BeautifulSoup(rq.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With .prettify, the code becomes much easier to read\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('table', class_=['infobox', 'vevent']).findAll('td', text=True)[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:57:16.326030Z",
     "start_time": "2020-12-02T13:57:16.321010Z"
    }
   },
   "source": [
    "### More practical example with BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import re\n",
    "\n",
    "rq = r.get('https://onemocneni-aktualne.mzcr.cz/pes')\n",
    "\n",
    "soup = BeautifulSoup(rq.text, 'html.parser')\n",
    "\n",
    "text = soup.find('span', id='pes-current-degree').get_text()\n",
    "match = re.match('.*(\\d).*', text)\n",
    "if match is not None:\n",
    "    print(f'Current degree is {match.groups()[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubles with dynamical pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = r.get('https://www.amazon.com/Apple-MacBook-13-inch-Storage-Keyboard/dp/B0882JG168/ref=sr_1_3?dchild=1&keywords=macbook+13+2020&qid=1606917636&sr=8-3')\n",
    "\n",
    "print(BeautifulSoup(rq.text, 'html.parser').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = r.get('https://coronavirus.app/tracking/czechia')\n",
    "\n",
    "print(BeautifulSoup(rq.text, 'html.parser').prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import urllib3\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "from url_normalize import url_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable InsecureRequestWarning\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Simple URL normalization\n",
    "# Converts relative URLs to absolute and converts to lowercase, then applies url-normalize\n",
    "# See (https://pypi.org/project/url-normalize/)\n",
    "def norm_url(url, base):\n",
    "    if url[0] == '/':\n",
    "        return url_normalize(urljoin(base, url).lower())\n",
    "    return url_normalize(url)\n",
    "\n",
    "# Retrieve the base of a URL\n",
    "def get_base_url(url):\n",
    "    split = urlsplit(url)\n",
    "    base = split.scheme + '://' + split.netloc + '/'\n",
    "    return base\n",
    "\n",
    "\n",
    "# Checks for outlinks in a HTML page\n",
    "def find_outlinks(soup):\n",
    "    # URL of the file is saved as the first comment of the HTML file.\n",
    "    base = get_base_url(soup.findAll(text = lambda text: isinstance(text, Comment))[0])\n",
    "    \n",
    "    all_urls = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Selects only URLs that are either absolute or relative to the domain.\n",
    "    selected_urls = [u for u in all_urls if len(u) > 1 and (u[0] == '/' and u[1] != '/' or u[0] == 'h')]\n",
    "    \n",
    "    # Normalizes selected URLs.\n",
    "    all_urls_normalized = [norm_url(u, base) for u in selected_urls]\n",
    "    \n",
    "    return all_urls_normalized\n",
    "\n",
    "# Finds robots.txt and processes its rules\n",
    "def process_robots(url):\n",
    "    base = get_base_url(url)\n",
    "    robots_file = base + 'robots.txt'\n",
    "    req = requests.get(robots_file)\n",
    "    \n",
    "    # If robots.txt cannot be processes, no rules are applied\n",
    "    if req.status_code != 200:\n",
    "        return []\n",
    "    \n",
    "    robots = req.text.split('\\n')\n",
    "    \n",
    "    # Only check for 'User-agent: *'\n",
    "    if 'User-agent: *' not in robots:\n",
    "        return []\n",
    "    \n",
    "    robots = robots[robots.index('User-agent: *')+1:]\n",
    "\n",
    "    # Retrieve all Disallow rules\n",
    "    rules = []\n",
    "    for rule in robots:\n",
    "        if rule.startswith('Disallow: '):\n",
    "            rules.append(rule.split(' ')[1])\n",
    "        if rule.startswith('User-agent: '):\n",
    "            break\n",
    "\n",
    "    # Normalize all URLs in the rules\n",
    "    rules_normalized = [norm_url(rule, base) for rule in rules]\n",
    "    \n",
    "    return rules_normalized\n",
    "\n",
    "# Check for visible text only\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif element == '\\n' or element == ' ':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_shingles(soup, n=4):\n",
    "    # Extract all textual data\n",
    "    data = soup.findAll(text=True)\n",
    "    \n",
    "    # Filter out comments, scripts, etc.\n",
    "    texts = filter(visible, data)\n",
    "    # Remove whitespaces\n",
    "    texts = [str(e).strip() for e in texts]\n",
    "    \n",
    "    # Convert to list of words\n",
    "    words = [text.split() for text in texts]\n",
    "    words = [w for sub in words for w in sub]\n",
    "    \n",
    "    shingles = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        shingles.append(tuple(words[i:i+n]))\n",
    "        \n",
    "    return set(shingles)\n",
    "\n",
    "def near_duplicate(shingles1, shingles2, threshold=0.8):\n",
    "    union = len(set(list(shingles1) + list(shingles2)))\n",
    "    overlap = len(list(shingles1) + list(shingles2)) - union\n",
    "\n",
    "    return overlap/union > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "n_pages = 10\n",
    "\n",
    "q = Queue()\n",
    "seed_url = \"https://en.wikipedia.org/wiki/Alexandria_Ocasio-Cortez\"\n",
    "\n",
    "q.put(seed_url)\n",
    "\n",
    "# Save texts of processed pages for near-duplication detection\n",
    "processed = []\n",
    "\n",
    "while i <= n_pages:\n",
    "    # Get URL from queue\n",
    "    url = q.get()\n",
    "\n",
    "    # Make a request to URL\n",
    "    try:\n",
    "        req = requests.get(url, verify=False, timeout=5)\n",
    "    except Exception:\n",
    "        continue\n",
    "    \n",
    "    # Check if page exists and is accessible\n",
    "    if req.status_code != 200:\n",
    "        continue\n",
    "        \n",
    "    soup = BeautifulSoup(req.text)\n",
    "    \n",
    "    # Extract 4-shingles out of textual data\n",
    "    shingles = extract_shingles(soup, n=4)\n",
    "    \n",
    "    # Check for near duplicates among processed files\n",
    "    if any([near_duplicate(shingles, processed_file, threshold=0.8) for processed_file in processed]):\n",
    "        continue\n",
    "    \n",
    "    # Save shingles for near-duplicate detection\n",
    "    processed.append(shingles)\n",
    "    \n",
    "    # Insert the URL as a comment on the first line of the created file\n",
    "    soup.insert(0, '\\n')\n",
    "    soup.insert(0, Comment(url))\n",
    "    \n",
    "    # Save the HTML of retrieved file\n",
    "    with open('Pages/{}'.format('{}.html'.format(i)), 'wb+') as file:\n",
    "        file.write(soup.prettify('utf-8'))\n",
    "                \n",
    "    # Find all outlinks in the file\n",
    "    urls = find_outlinks(soup)\n",
    "    # Retrieve rules from robots.txt\n",
    "    rules = process_robots(url)\n",
    "    \n",
    "    # Use only outlinks that aren't contradictory to rules from robots.txt\n",
    "    urls_to_add = []\n",
    "    for u in urls:\n",
    "        for rule in rules:\n",
    "            if rule.startswith(u):\n",
    "                continue\n",
    "        urls_to_add.append(u)\n",
    "    \n",
    "    # Put filtered outlinks to queue\n",
    "    for u in urls_to_add:\n",
    "        q.put(u)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "with open(glob('Pages/*')[0]) as f:\n",
    "    soup = BeautifulSoup(f)\n",
    "    print(soup.find('title').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('./chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Load the Chromedriver and open a browser\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "# go to a page\n",
    "driver.get('https://www.amazon.com/')\n",
    "\n",
    "# Find the search box and write text to it\n",
    "driver.find_element_by_id('twotabsearchtextbox').send_keys('macbook pro 2020')\n",
    "\n",
    "# Find the search box and press enter\n",
    "driver.find_element_by_id('twotabsearchtextbox').send_keys(u'\\ue007')\n",
    "\n",
    "# Locate the first product's image and click on it\n",
    "driver.find_elements_by_class_name('s-image')[0].click()\n",
    "\n",
    "# Finally, retrieve the price\n",
    "driver.find_element_by_id('priceblock_ourprice').text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
